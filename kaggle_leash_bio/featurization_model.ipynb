{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#neccessary imports\n",
    "import pandas as pd\n",
    "import pyarrow.dataset as ds\n",
    "from loguru import logger\n",
    "import wandb\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Linear\n",
    "import tensorflow as tf\n",
    "import dgl\n",
    "from dgl import batch as dgl_batch\n",
    "from rdkit import Chem\n",
    "from rdkit.Chem import AllChem\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from dgl.nn import GraphConv, GlobalAttentionPooling\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "print(\"PyTorch version:\", torch.__version__)\n",
    "print(\"DGL version:\", dgl.__version__)\n",
    "print(\"CUDA available in PyTorch:\", torch.cuda.is_available())\n",
    "\n",
    "# Check CUDA support in DGL\n",
    "try:\n",
    "    # Create a simple graph\n",
    "    g = dgl.graph((torch.tensor([0, 1]), torch.tensor([1, 2])))\n",
    "\n",
    "    # Try to move the graph to GPU\n",
    "    g = g.to('cuda')\n",
    "    print(\"CUDA available in DGL: True\")\n",
    "except:\n",
    "    print(\"CUDA available in DGL: False\")\n",
    "print(dgl.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_combined = ds.dataset(source=\"../../../data/train_combined.parquet\", format=\"parquet\")\n",
    "# Convert the PyArrow table to a Pandas dataframe\n",
    "df = data_combined.to_table().to_pandas()\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define a function to split the data for training and testing \n",
    "# keeping the distribution of the target variable the same in both sets and the bind vs no bind ratio the same\n",
    "def split_data(df, target_col, test_size=0.2, random_state=42):\n",
    "    df_train, df_test = train_test_split(df, test_size=test_size, random_state=random_state, stratify=df[target_col])\n",
    "    return df_train, df_test\n",
    "\n",
    "\n",
    "train_brd4,test_brd4=split_data(df[df['protein_name']=='BRD4'], \"binds\")\n",
    "train_hsa,test_hsa=split_data(df[df['protein_name']=='HSA'], \"binds\")\n",
    "train_seh,test_seh=split_data(df[df['protein_name']=='sEH'], \"binds\")\n",
    "\n",
    "logger.info(f\"Train set BRD4 {len(train_brd4)}\")\n",
    "logger.info(f\"Test set BRD4 {len(test_brd4)}\")\n",
    "logger.info(f\"Train set HSA {len(train_hsa)}\")\n",
    "logger.info(f\"Test set HSA {len(test_hsa)}\")\n",
    "logger.info(f\"Train set sEH {len(train_seh)}\")\n",
    "logger.info(f\"Test set sEH {len(test_seh)}\")\n",
    "\n",
    "logger.info(f\"Train set bind ratio BRD4 {len(train_brd4[train_brd4['binds']==1])/len(train_brd4)}\")\n",
    "logger.info(f\"Test set bind ratio BRD4 {len(test_brd4[test_brd4['binds']==1])/len(test_brd4)}\")\n",
    "logger.info(f\"Train set bind ratio HSA {len(train_hsa[train_hsa['binds']==1])/len(train_hsa)}\")\n",
    "logger.info(f\"Test set bind ratio HSA {len(test_hsa[test_hsa['binds']==1])/len(test_hsa)}\")\n",
    "logger.info(f\"Train set bind ratio sEH {len(train_seh[train_seh['binds']==1])/len(train_seh)}\")\n",
    "logger.info(f\"Test set bind ratio sEH {len(test_seh[test_seh['binds']==1])/len(test_seh)}\")\n",
    "\n",
    "train_combined = pd.concat([train_brd4, train_hsa, train_seh])\n",
    "test_combined = pd.concat([test_brd4, test_hsa, test_seh])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_smi(smi: str | list):\n",
    "    r\"\"\" Clean a SMILES string by removing salts and fragments.\n",
    "    Parameters\n",
    "    ----------\n",
    "    smi : str | list\n",
    "        The SMILES string for a molecule. or a list of SMILES strings\n",
    "    Returns\n",
    "    -------\n",
    "    str | list\n",
    "        The cleaned SMILES string.\n",
    "    \"\"\"\n",
    "    if isinstance(smi, list):\n",
    "        return [clean_smi(s) for s in smi]\n",
    "    # Remove [Dy] from smiles\n",
    "    smi = smi.replace(\"[Dy]\", \"\")\n",
    "\n",
    "    # Convert SMILES to a RDKit molecule object\n",
    "    mol = Chem.MolFromSmiles(smi)\n",
    "    if mol is None:\n",
    "        raise ValueError(\"Invalid SMILES string\")\n",
    "    \n",
    "    # Remove any salts or fragments\n",
    "    mol = Chem.RemoveHs(mol)  # Remove explicit hydrogens\n",
    "    fragments = Chem.GetMolFrags(mol, asMols=True)\n",
    "    \n",
    "    # Keep the largest fragment\n",
    "    largest_fragment = max(fragments, default=mol, key=lambda m: m.GetNumAtoms())\n",
    "    \n",
    "    # Standardize the molecule\n",
    "    AllChem.Compute2DCoords(largest_fragment)  # Compute 2D coordinates\n",
    "    \n",
    "    # Convert the molecule back to a canonical SMILES string\n",
    "    cleaned_smiles = Chem.MolToSmiles(largest_fragment, canonical=True)\n",
    "    return cleaned_smiles\n",
    "\n",
    "\n",
    "def smiles_to_dgl_graph(smiles: str |list):\n",
    "    r\"\"\" Convert a SMILES string to a DGLGraph.\n",
    "    Parameters\n",
    "    ----------\n",
    "    smiles : str | list\n",
    "        The SMILES string for a molecule. or a list of SMILES strings\n",
    "    Returns\n",
    "    -------\n",
    "    DGLGraph\n",
    "        A DGLGraph object for the molecule.\n",
    "    \"\"\"\n",
    "    if isinstance(smiles, list):\n",
    "        return [smiles_to_dgl_graph(s) for s in smiles]\n",
    "    clean_smiles=clean_smi(smiles)\n",
    "    mol = Chem.MolFromSmiles(clean_smiles)\n",
    "    if mol is None:\n",
    "        return None\n",
    "\n",
    "    # Node features\n",
    "    atom_features = []\n",
    "    for atom in mol.GetAtoms():\n",
    "        atom_features.append([\n",
    "            atom.GetAtomicNum(),\n",
    "            atom.GetDegree(),\n",
    "            atom.GetFormalCharge(),\n",
    "            atom.GetHybridization(),\n",
    "            atom.GetIsAromatic(),\n",
    "            atom.GetTotalNumHs()\n",
    "        ])\n",
    "    \n",
    "    # Edge features and adjacency list\n",
    "    src, dst = [], []\n",
    "    bond_features = []\n",
    "    for bond in mol.GetBonds():\n",
    "        start, end = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        src.append(start)\n",
    "        dst.append(end)\n",
    "        bond_features.append([\n",
    "            int(bond.GetBondTypeAsDouble()), \n",
    "            bond.GetBondType(),\n",
    "            bond.GetIsConjugated(),\n",
    "            bond.IsInRing()\n",
    "        ])\n",
    "    \"\"\"\n",
    "    g = dgl.graph((src, dst))\n",
    "    g.ndata['h'] = torch.tensor(atom_features, dtype=torch.float)\n",
    "    g.edata['h'] = torch.tensor(bond_features, dtype=torch.float)\n",
    "    g= dgl.add_self_loop(g)\n",
    "    return g\n",
    "    \"\"\"\n",
    "    edge_index = torch.tensor([src, dst], dtype=torch.long)\n",
    "    node_features = torch.tensor(atom_features, dtype=torch.float)\n",
    "    edge_features = torch.tensor(bond_features, dtype=torch.float)\n",
    "\n",
    "    data = Data(x=node_features, edge_index=edge_index, edge_attr=edge_features)\n",
    "    return data\n",
    "\n",
    "#create a dictionary with the protein names as keys and the protein sequences as values\n",
    "protein_sequences = {\n",
    "    \"BRD4\": \"NPPPPETSNPNKPKRQTNQLQYLLRVVLKTLWKHQFAWPFQQPVDAVKLNLPDYYKIIKTPMDMGTIKKRLENNYYWNAQECIQDFNTMFTNCYIYNKPGDDIVLMAEALEKLFLQKINELPTEETEIMIVQAKGRGRGRKETGTAKPGVSTVPNTTQASTPPQTQTPQPNPPPVQATPHPFPAVTPDLIVQTPVMTVVPPQPLQTPPPVPPQPQPPPAPAPQPVQSHPPIIAATPQPVKTKKGVKRKADTTTPTTIDPIHEPPSLPPEPKTTKLGQRRESSRPVKPPKKDVPDSQQHPAPEKSSKVSEQLKCCSGILKEMFAKKHAAYAWPFYKPVDVEALGLHDYCDIIKHPMDMSTIKSKLEAREYRDAQEFGADVRLMFSNCYKYNPPDHEVVAMARKLQDVFEMRFAKMPDE\",\n",
    "    \"sEH\": \"TLRAAVFDLDGVLALPAVFGVLGRTEEALALPRGLLNDAFQKGGPEGATTRLMKGEITLSQWIPLMEENCRKCSETAKVCLPKNFSIKEIFDKAISARKINRPMLQAALMLRKKGFTTAILTNTWLDDRAERDGLAQLMCELKMHFDFLIESCQVGMVKPEPQIYKFLLDTLKASPSEVVFLDDIGANLKPARDLGMVTILVQDTDTALKELEKVTGIQLLNTPAPLPTSCNPSDMSHGYVTVKPRVRLHFVELGSGPAVCLCHGFPESWYSWRYQIPALAQAGYRVLAMDMKGYGESSAPPEIEEYCMEVLCKEMVTFLDKLGLSQAVFIGHDWGGMLVWYMALFYPERVRAVASLNTPFIPANPNMSPLESIKANPVFDYQLYFQEPGVAEAELEQNLSRTFKSLFRASDESVLSMHKVCEAGGLFVNSPEEPSLSRMVTEEEIQFYVQQFKKSGFRGPLNWYRNMERNWKWACKSLGRKILIPALMVTAEKDFVLVPQMSQHMEDWIPHLKRGHIEDCGHWTQMDKPTEVNQILIKWLDSDARNPPVVSKM\",\n",
    "    \"HSA\": \"DAHKSEVAHRFKDLGEENFKALVLIAFAQYLQQCPFEDHVKLVNEVTEFAKTCVADESAENCDKSLHTLFGDKLCTVATLRETYGEMADCCAKQEPERNECFLQHKDDNPNLPRLVRPEVDVMCTAFHDNEETFLKKYLYEIARRHPYFYAPELLFFAKRYKAAFTECCQAADKAACLLPKLDELRDEGKASSAKQRLKCASLQKFGERAFKAWAVARLSQRFPKAEFAEVSKLVTDLTKVHTECCHGDLLECADDRADLAKYICENQDSISSKLKECCEKPLLEKSHCIAEVENDEMPADLPSLAADFVESKDVCKNYAEAKDVFLGMFLYEYARRHPDYSVVLLLRLAKTYETTLEKCCAAADPHECYAKVFDEFKPLVEEPQNLIKQNCELFEQLGEYKFQNALLVRYTKKVPQVSTPTLVEVSRNLGKVGSKCCKHPEAKRMPCAEDYLSVVLNQLCVLHEKTPVSDRVTKCCTESLVNRRPCFSALEVDETYVPKEFNAETFTFHADICTLSEKERQIKKQTALVELVKHKPKATKEQLKAVMDDFAAFVEKCCKADDKETCFAEEGKKLVAASQAALGL\",\n",
    "}\n",
    "\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "aa_to_index = {aa: idx for idx, aa in enumerate(amino_acids)}\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, aa_to_index=aa_to_index):\n",
    "    one_hot_encoded = np.zeros((len(sequence), len(amino_acids)), dtype=np.float32)\n",
    "    for i, aa in enumerate(sequence):\n",
    "        if aa in aa_to_index:\n",
    "            one_hot_encoded[i, aa_to_index[aa]] = 1.0\n",
    "    return one_hot_encoded\n",
    "\n",
    "# Function to generate one-hot encoded features for proteins\n",
    "def generate_protein_one_hot_features(protein_sequences):\n",
    "    features = {}\n",
    "    for protein_name, sequence in protein_sequences.items():\n",
    "        one_hot_features = one_hot_encode_sequence(sequence)\n",
    "        features[protein_name] = one_hot_features\n",
    "    return features\n",
    "\n",
    "def protein_to_features(protein_seq: str):\n",
    "    #one hot encode the amino acid composition\n",
    "    one_hot_encoded = one_hot_encode_sequence(protein_seq)\n",
    "    amino_acid_composition = torch.tensor(one_hot_encoded, dtype=torch.float)\n",
    "    # Hydrophobicity\n",
    "    hydrophobicity = [0] * len(protein_seq)\n",
    "    for i, amino_acid in enumerate(protein_seq):\n",
    "        if amino_acid in ['A', 'I', 'L', 'M', 'F', 'W', 'V']:\n",
    "            hydrophobicity[i] = 1\n",
    "    hydrophobicity = torch.tensor(hydrophobicity, dtype=torch.float).unsqueeze(1)\n",
    "    # Charge\n",
    "    charge = [0] * len(protein_seq)\n",
    "    for i, amino_acid in enumerate(protein_seq):\n",
    "        if amino_acid in ['K', 'R']:\n",
    "            charge[i] = 1\n",
    "        elif amino_acid in ['D', 'E']:\n",
    "            charge[i] = -1\n",
    "    charge = torch.tensor(charge, dtype=torch.float).unsqueeze(1)\n",
    "    # Concatenate features\n",
    "    protein_features = torch.cat((amino_acid_composition, hydrophobicity, charge), dim=1)\n",
    "    return protein_features\n",
    "\n",
    "def protein_to_graph(protein_seq, protein_features, neighbor_distance=3):\n",
    "    node_features = protein_features\n",
    "    edge_index = []\n",
    "    for i in range(len(protein_seq)):\n",
    "        for j in range(i + 1, min(i + neighbor_distance + 1, len(protein_seq))):\n",
    "            edge_index.append([i, j])\n",
    "            edge_index.append([j, i])\n",
    "    edge_index = torch.tensor(edge_index, dtype=torch.long).t()\n",
    "\n",
    "    data = Data(x=node_features, edge_index=edge_index)\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_add_pool, global_mean_pool, global_max_pool\n",
    "\n",
    "class ProteinLigandGCN(torch.nn.Module):\n",
    "    def __init__(self, protein_feature_dim, ligand_feature_dim, hidden_dim, output_dim,dropout=0.2):\n",
    "        super(ProteinLigandGCN, self).__init__()\n",
    "        # Initialize GCN layers for protein\n",
    "        self.protein_conv1 = GCNConv(protein_feature_dim, hidden_dim)\n",
    "        self.protein_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Initialize GCN layers for ligand\n",
    "        self.ligand_conv1 = GCNConv(ligand_feature_dim, hidden_dim)\n",
    "        self.ligand_conv2 = GCNConv(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Dropout layer\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "        # Fully connected layers for each graph after GCN layers\n",
    "        self.protein_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.ligand_fc = nn.Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Final fully connected layer after combining\n",
    "        self.final_fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, protein_data, ligand_data):\n",
    "        # Protein graph processing\n",
    "        protein_graph, protein_features = protein_data\n",
    "        ligand_graph, ligand_features = ligand_data\n",
    "        \n",
    "        # Apply GCN layers to protein graph\n",
    "        protein_x = F.relu(self.protein_conv1(protein_features, protein_graph.edge_index))\n",
    "        protein_x = self.dropout1(protein_x)\n",
    "        protein_x = F.relu(self.protein_conv2(protein_x, protein_graph.edge_index))\n",
    "        protein_x = self.dropout2(protein_x)\n",
    "        \n",
    "        # Pooling (choose one of the following or implement your own)\n",
    "        #protein_output = global_add_pool(protein_x, protein_graph.batch)  # Global sum pooling\n",
    "        protein_output = global_mean_pool(protein_x, protein_graph.batch)  # Global mean pooling\n",
    "        # protein_output = global_max_pool(protein_x, protein_graph.batch)  # Global max pooling\n",
    "        \n",
    "        protein_output = F.relu(self.protein_fc(protein_output))\n",
    "\n",
    "        # Apply GCN layers to ligand graph\n",
    "        ligand_x = F.relu(self.ligand_conv1(ligand_features, ligand_graph.edge_index))\n",
    "        ligand_x = self.dropout1(ligand_x)\n",
    "        ligand_x = F.relu(self.ligand_conv2(ligand_x, ligand_graph.edge_index))\n",
    "        ligand_x = self.dropout2(ligand_x)\n",
    "        \n",
    "        # Pooling (choose one of the following or implement your own)\n",
    "        #ligand_output = global_add_pool(ligand_x, ligand_graph.batch)  # Global sum pooling\n",
    "        ligand_output = global_mean_pool(ligand_x, ligand_graph.batch)  # Global mean pooling\n",
    "        # ligand_output = global_max_pool(ligand_x, ligand_graph.batch)  # Global max pooling\n",
    "        \n",
    "        ligand_output = F.relu(self.ligand_fc(ligand_output))\n",
    "\n",
    "        # Combine protein and ligand outputs\n",
    "        combined_output = torch.cat((protein_output, ligand_output), dim=1)\n",
    "        final_output = self.final_fc(combined_output)\n",
    "\n",
    "        return final_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch_geometric.data import Batch\n",
    "class ProteinLigandDataset(Dataset):\n",
    "    def __init__(self, data, protein_sequences, transform=None):\n",
    "        self.data = data\n",
    "        self.protein_sequences = protein_sequences\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        protein_name = self.data.iloc[idx][\"protein_name\"]\n",
    "        molecule_smiles = self.data.iloc[idx][\"molecule_smiles\"]\n",
    "        #check if there is a label in the data\n",
    "        if \"binds\" in self.data.columns:\n",
    "            label = self.data.iloc[idx][\"binds\"]\n",
    "        else:\n",
    "            label = None\n",
    "        #label = self.data.iloc[idx][\"binds\"]\n",
    "        \n",
    "        protein_graph = protein_to_graph(self.protein_sequences[protein_name], protein_to_features(self.protein_sequences[protein_name]))\n",
    "        smiles_graph = smiles_to_dgl_graph(molecule_smiles)\n",
    "        \"\"\"\n",
    "        protein_features = protein_graph.ndata['h']\n",
    "        smiles_features = smiles_graph.ndata['h']\n",
    "                    'protein_features': protein_features,\n",
    "                                'smiles_features': smiles_features,\n",
    "        \"\"\"\n",
    "        sample = {\n",
    "            'protein_graph': protein_graph,\n",
    "            'smiles_graph': smiles_graph,\n",
    "            'label': torch.tensor(label, dtype=torch.float32) if label is not None else None\n",
    "        }\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        \n",
    "        return sample\n",
    "\n",
    "\n",
    "def custom_collate_fn(batch):\n",
    "    protein_graphs = [item['protein_graph'] for item in batch]\n",
    "    #protein_features = [item['protein_features'] for item in batch]\n",
    "    smiles_graphs = [item['smiles_graph'] for item in batch]\n",
    "    #smiles_features = [item['smiles_features'] for item in batch]\n",
    "    labels = torch.tensor([item['label'] for item in batch], dtype=torch.float32)\n",
    "    \"\"\"\n",
    "    batched_protein_graph = dgl_batch(protein_graphs)\n",
    "    batched_smiles_graph = dgl_batch(smiles_graphs)\n",
    "    \n",
    "    batched_protein_features = torch.cat(protein_features)\n",
    "    batched_smiles_features = torch.cat(smiles_features)\n",
    "    \"\"\"\n",
    "    batched_protein_graph = Batch.from_data_list(protein_graphs)\n",
    "    batched_smiles_graph = Batch.from_data_list(smiles_graphs)\n",
    "    return {\n",
    "        'protein_graph': batched_protein_graph,\n",
    "        'smiles_graph': batched_smiles_graph,\n",
    "        'label': labels if labels[0] is not None else None\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the datasets\n",
    "\n",
    "batch_size = 128\n",
    "train_dataset = ProteinLigandDataset(train_combined, protein_sequences)\n",
    "val_dataset = ProteinLigandDataset(test_combined, protein_sequences)\n",
    "# Initialize the dataloaders with custom collate function\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "val_loader = DataLoader(val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get 100,000 samples from the train_combined for each protein where binds=1\n",
    "train_combined_pos=train_combined[train_combined['binds']==1]\n",
    "train_combined_neg=train_combined[train_combined['binds']==0]\n",
    "\n",
    "#get 100,000 samples from the train_combined for brd4 where binds=1 and 100,000 where binds=0\n",
    "train_brd4_pos=train_combined_pos[train_combined_pos['protein_name']=='BRD4'].sample(n=100000)\n",
    "train_brd4_neg=train_combined_neg[train_combined_neg['protein_name']=='BRD4'].sample(n=100000)\n",
    "\n",
    "#get 100,000 samples from the train_combined for hsa where binds=1 and 100,000 where binds=0\n",
    "train_hsa_pos=train_combined_pos[train_combined_pos['protein_name']=='HSA'].sample(n=100000)\n",
    "train_hsa_neg=train_combined_neg[train_combined_neg['protein_name']=='HSA'].sample(n=100000)\n",
    "\n",
    "\n",
    "#get 100,000 samples from the train_combined for seh where binds=1 and 100,000 where binds=0\n",
    "train_seh_pos=train_combined_pos[train_combined_pos['protein_name']=='sEH'].sample(n=100000)\n",
    "train_seh_neg=train_combined_neg[train_combined_neg['protein_name']=='sEH'].sample(n=100000)\n",
    "\n",
    "train_combined_sampled = pd.concat([train_brd4_pos, train_brd4_neg, train_hsa_pos, train_hsa_neg, train_seh_pos, train_seh_neg])\n",
    "\n",
    "#do the same for the test_combined\n",
    "test_combined_pos=test_combined[test_combined['binds']==1]\n",
    "test_combined_neg=test_combined[test_combined['binds']==0]\n",
    "\n",
    "#get 100,000 samples from the test_combined for brd4 where binds=1 and 100,000 where binds=0\n",
    "test_brd4_pos=test_combined_pos[test_combined_pos['protein_name']=='BRD4'].sample(n=10000)\n",
    "test_brd4_neg=test_combined_neg[test_combined_neg['protein_name']=='BRD4'].sample(n=10000)\n",
    "\n",
    "#get 100,000 samples from the test_combined for hsa where binds=1 and 100,000 where binds=0\n",
    "test_hsa_pos=test_combined_pos[test_combined_pos['protein_name']=='HSA'].sample(n=10000)\n",
    "test_hsa_neg=test_combined_neg[test_combined_neg['protein_name']=='HSA'].sample(n=10000)\n",
    "\n",
    "test_seh_pos=test_combined_pos[test_combined_pos['protein_name']=='sEH'].sample(n=10000)\n",
    "test_seh_neg=test_combined_neg[test_combined_neg['protein_name']=='sEH'].sample(n=10000)\n",
    "\n",
    "test_combined_sampled = pd.concat([test_brd4_pos, test_brd4_neg, test_hsa_pos, test_hsa_neg, test_seh_pos, test_seh_neg])\n",
    "\n",
    "# Initialize the datasets\n",
    "sampled_train_dataset = ProteinLigandDataset(train_combined_sampled, protein_sequences)\n",
    "sampled_val_dataset = ProteinLigandDataset(test_combined_sampled, protein_sequences)\n",
    "\n",
    "\n",
    "# Initialize the dataloaders with custom collate function\n",
    "sampled_train_loader = DataLoader(sampled_train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
    "sampled_val_loader = DataLoader(sampled_val_dataset, batch_size=1, shuffle=False, collate_fn=custom_collate_fn)\n",
    "\n",
    "#print(test_combined_sampled.tail())\n",
    "#print(train_combined_sampled.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_step = 0\n",
    "batch_size = 128\n",
    "learning_rate = 0.1\n",
    "wandb.init(project=\"BELKA_NeruIPS\", entity=\"mayarahmed\",config={\"learning_rate\":learning_rate,\"architecture\":\"GCN\",\"dataset\":\"BELKA\",\"epochs\":100,\"batch_size\":batch_size})\n",
    "logger.info(\"Initialized WandB run\")\n",
    "logger.info(f'batch size: {batch_size}')\n",
    "protein_node_feats = 22\n",
    "smiles_node_feats = 6\n",
    "edge_feats = 4\n",
    "hidden_dim = 128\n",
    "output_dim = 1\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "logger.info(f\"Using {device} device\")\n",
    "# Initialize the model\n",
    "model = ProteinLigandGCN(protein_node_feats, smiles_node_feats, hidden_dim, output_dim).to(device)\n",
    "\n",
    "\n",
    "checkpoint = torch.load('best_model_30.pt')\n",
    "\n",
    "wandb.watch(model, log_freq=2) \n",
    "logger.info(checkpoint.keys())\n",
    "model.load_state_dict(checkpoint['odel_state_dict'])\n",
    "logger.info(\"Loaded model state from checkpoint\")\n",
    "penalty=torch.tensor([len(train_combined[train_combined['binds']==0])/len(train_combined[train_combined['binds']==1])] ,dtype=torch.float32).to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=penalty)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "#scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
    "#scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer,eta_min=0.0001,T_0=10,T_mult=2)\n",
    "scheduler=torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.25, patience=5)\n",
    "\n",
    "\n",
    "if 'optimizer_state_dict' in checkpoint:\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    logger.info(\"Loaded optimizer state from checkpoint\")\n",
    "model.train()\n",
    "\n",
    "# Train the model\n",
    "best_loss = float('inf')\n",
    "patience = 5  # number of epochs to wait before stopping training\n",
    "patience_counter = 0\n",
    "min_delta = 0.001  # minimum difference between train and val loss to consider convergence\n",
    "\n",
    "for epoch in range(1,100):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    optimizer.zero_grad()\n",
    "    for batch_ix, batch in enumerate(train_loader):\n",
    "        protein_graph = batch['protein_graph'].to(device)\n",
    "        #get the protein features from the protein graph\n",
    "        protein_features = protein_graph.x.to(device)\n",
    "        #protein_features = batch['protein_features'].to(device)\n",
    "        smiles_graph = batch['smiles_graph'].to(device)\n",
    "        #smiles_features = batch['smiles_features'].to(device)\n",
    "        smiles_features = smiles_graph.x.to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass\n",
    "        output = model((protein_graph, protein_features), (smiles_graph, smiles_features))\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output.squeeze(1), labels)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Log the loss and accumulate train loss\n",
    "        wandb.log({\"epoch\": epoch, \"train_loss\": loss.item(), \"trainer/global_step\": global_step,\"batch\":batch_ix,\"learning_rate\":optimizer.param_groups[0]['lr']})\n",
    "        train_loss += loss.item()\n",
    "        global_step += 1\n",
    "    \n",
    "    # Calculate average train loss\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    # Evaluate on validation set\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            protein_graph = batch['protein_graph'].to(device)\n",
    "            #protein_features = batch['protein_features'].to(device)\n",
    "            protein_features=protein_graph.x.to(device)\n",
    "            smiles_graph = batch['smiles_graph'].to(device)\n",
    "            #smiles_features = batch['smiles_features'].to(device)\n",
    "            smiles_features=smiles_graph.x.to(device)\n",
    "            labels = batch['label'].to(device)        \n",
    "            output = model((protein_graph, protein_features), (smiles_graph, smiles_features))\n",
    "            val_loss_tmp=criterion(output.squeeze(1), labels).item()\n",
    "            val_loss += val_loss_tmp\n",
    "            wandb.log({\"val_loss\":val_loss_tmp, \"trainer/global_step\": global_step,'epoch':epoch})\n",
    "\n",
    "        val_loss /= len(val_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "    # Check if the model is converging\n",
    "    if abs(train_loss - val_loss) < min_delta and val_loss < 0.5:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            logger.info(\"Model is converging. Stopping training.\")\n",
    "            break\n",
    "    else:\n",
    "        patience_counter = 0\n",
    "\n",
    "    # Log the loss to WandB\n",
    "    wandb.log({\"average_train_loss\": train_loss, \"average_val_loss\": val_loss, \"epoch\": epoch})\n",
    "\n",
    "    # Save the model with the lowest loss\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        # Save the model and optimizer state\n",
    "        torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'average_train_loss': train_loss,\n",
    "        'average_val_loss': val_loss,\n",
    "        }, f'best_model_{str(epoch)}.pt')\n",
    "\n",
    "        wandb.save(f\"best_model_{str(epoch)}.pt\")\n",
    "\n",
    "    # Print the loss\n",
    "    logger.info(f'Epoch {epoch}, Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "\n",
    "# Finish the WandB run\n",
    "wandb.finish()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leash_bio_kaggle-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
